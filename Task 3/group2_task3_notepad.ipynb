{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Modeling Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Course:** Introduction to Data Science\n",
    "**Lecturer:** Prof. Dr. Hendrik Meth\n",
    "\n",
    "**Group 2:**\n",
    "- Linus Breitenberger\n",
    "- Tristan Ruhm\n",
    "- Prarichut Poachanuan\n",
    "- Anushka Irphale\n",
    "- Patryk Gadziosmki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:30px;background-color:#E31134\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:30px;background-color:#E31134\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary of Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reworking and Optimization of train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our final model for Task 2 didn't seem to be optimal and some details had been overseen, we decided to do a rework of our train and test datas and optimize it.\n",
    "\n",
    "Because of the high correlation with 'cnt', we decided to keep 'instant' in the data set. The features 'temp' and 'atemp' had a multicorrelation, so we kept 'atemp', which had a minimal better correlation, in and dropped 'temp'. We also removed the 'casual' and 'registered' labels, since we only use 'cnt'.\n",
    "\n",
    "In the end we tested our models with the new test data and had following result:\n",
    "- MAE: 923.977 \n",
    "- R^2 value of the model:  0.36870153666640637\n",
    "\n",
    "The Mean Absolute Error has an improvement. On the opposite, the determination R^2 has a deterioration. We exported the reworked train- and testdata to csv and used for the model optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first step, we refined the feature and label for our model buidling with splitting the training data into `train_features` and `train_labels` and the testdata into `test_features` and `test_label`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better model optimization, we created further model versions. Beside of linear regression, scikit-learn has various other algorithms. With testing these algortihms, we can figure out, which model has the best result. In the following, we present the results of each algorithm.\n",
    "\n",
    "For each model, we used the method forward selection, to get the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initiated a model with the algorithm polynomial regression.\n",
    "\n",
    "For that, we used the function `makepipeline(PolynomialFeatures())`. It gives out multiple results, which  will run through `linear_model.LinearRegression()`, to get one value as a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 K-Nearest-neighbours Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the K-nearest-neighbours regression, we compared each value of 'cnt' with his 10 nearest neighbour-values. For that, we used the function `KNeighborsRegressor()` to predict a numeric label, which is the average of all 10 values. A special aspect is that no linear relationships are required to build a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Regression Tree / Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regression tree makes it possible to handle nominal features and non-linear relationships. We used the `DecisionTreeRegressor()`-function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Regression defines a hyperplane, which can be linear, polynomial or gaussian.\n",
    "\n",
    "First, we used the function `SVR()` with a polynomial kernel, since the evaluation of polynomial regression had a good result. The result of it was not so good, so we concluded to take the linear kernel, which had a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Interpretation & Conlcusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
